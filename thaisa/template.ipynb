{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "y_test.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot all features same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.boxplot(data=df, orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers ,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "def scale_features(X_train, X_test, type='standard'):\n",
    "    \"\"\"\n",
    "    Scale features using various scalers.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "    - type (str): Type of scaler ('standard', 'robust', 'minmax').\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Scaled X_train and X_test.\n",
    "    \"\"\"\n",
    "    if type == 'standard':\n",
    "        sc_X = StandardScaler()\n",
    "        X_train_scaled = sc_X.fit_transform(X_train)\n",
    "        X_test_scaled = sc_X.transform(X_test)\n",
    "    elif type == 'robust':\n",
    "        sc_X = RobustScaler()\n",
    "        X_train_scaled = sc_X.fit_transform(X_train)\n",
    "        X_test_scaled = sc_X.transform(X_test)\n",
    "    elif type == 'minmax':\n",
    "        sc_X = MinMaxScaler()\n",
    "        X_train_scaled = sc_X.fit_transform(X_train)\n",
    "        X_test_scaled = sc_X.transform(X_test)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling type. Use 'standard', 'robust', or 'minmax'.\")\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode \"Extracurricular Activities\" using sklearn LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "X_train[\"Extracurricular Activities\"] = lb_make.fit_transform(X_train[\"Extracurricular Activities\"])\n",
    "X_test[\"Extracurricular Activities\"] = lb_make.transform(X_test[\"Extracurricular Activities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "def plot_learning_curves(model, X_train, y_train, X_test, y_test, model_type, c):\n",
    "\n",
    "  train_errors, test_errors = [], []\n",
    "\n",
    "  for m in range(1, len(X_train)):\n",
    "\n",
    "    model.fit(X_train[:m], y_train[:m])\n",
    "    y_train_predict = model.predict(X_train[:m])\n",
    "    y_test_predict = model.predict(X_test)\n",
    "\n",
    "    train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
    "\n",
    "  plt.plot(np.sqrt(train_errors), 'b', linewidth=2, label=\"train_\"+model_type)\n",
    "  plt.plot(np.sqrt(test_errors), 'g' , linewidth=3, label=\"test_\"+model_type)\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.01, solver=\"cholesky\")\n",
    "plot_learning_curves(ridge_reg, X_train_poly, y_train, X_test_poly, y_test, \"Ridge\", 'b')\n",
    "\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RIDGE Regression, learning curve (alpha=0.01)')\n",
    "plt.legend()\n",
    "\n",
    "# Usage example\n",
    "# --------------\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# lin_reg = LinearRegression()\n",
    "# plot_learning_curves(lin_reg, X_train_poly, y_train, X_test_poly, y_test, \"LinearRegression\", 'g')\n",
    "# plt.title('Linear Regression, learning curve')\n",
    "# plt.xlabel('Training set size')\n",
    "# plt.ylabel('RMSE')\n",
    "# plt.ylim((0,80))\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#feature scalining is done already by the library\n",
    "# Feature Scaling\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\"\"\"\n",
    "\n",
    "\n",
    "#fitting linear model on the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def simple_lin_reg(x_train, y_train):\n",
    "    regressor = LinearRegression()    \n",
    "    regressor.fit(X_train, y_train)  \n",
    "    return regressor\n",
    "\n",
    "#predecting the test result\n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "# vector of predictions of y \n",
    "# y_pred = regressor.predict(X_test)       \n",
    "y_pred = regressor.predict(theta_best_svd)       \n",
    "\n",
    "print(\"test RMSE={}\".format(np.sqrt(mean_squared_error(y_test, y_predict))))\n",
    "print(\"test R2={}\".format(r2_score(y_test, y_predict)))\n",
    "\n",
    "print(\"train RMSE={}\".format(np.sqrt(mean_squared_error(y_train, regressor.predict(X_train_poly)))))\n",
    "print(\"train R2={}\".format(r2_score(y_train, lin_reg.predict(X_train_poly))))\n",
    "\n",
    "#visualizing the training set results\n",
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title('Salary vs Experience (trainings set)')\n",
    "plt.xlabel ('Year of experience')\n",
    "plt.ylabel ('Salary')\n",
    "plt.show()   \n",
    "\n",
    "#visualizing the test set results\n",
    "plt.scatter(X_test, y_test, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title('Salary vs Experience (test set)')\n",
    "plt.xlabel ('Year of experience')\n",
    "plt.ylabel ('Salary')\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search on different n and find best accuracy\n",
    "eta_values = np.round(np.arange(0,0.8,0.05),2)# learning rate\n",
    "n_iterations = 100\n",
    "\n",
    "r2_scores = []\n",
    "rmse_test_GD, rmse_train_GD = [], []\n",
    "m=X_train.shape[0]\n",
    "\n",
    "for eta in eta_values:\n",
    "  print(\"\\neta = {}\".format(eta))\n",
    "  theta = np.random.randn(X_train.shape[1]+1,1).reshape(1,-1)[0]  # coefficients random initialization\n",
    "\n",
    "  for iteration in range(n_iterations):\n",
    "    gradients = (2/m) * X_b_train.T.dot(X_b_train.dot(theta) - y_train)\n",
    "    theta = theta - eta * gradients\n",
    "    X_b_test = np.c_[np.ones((len(X_test), 1)), X_test] # add x0 = 1 to each instance\n",
    "    y_pred_test = X_b_test.dot(theta)\n",
    "    y_pred_train = X_b_train.dot(theta)\n",
    "\n",
    "  rmse_test_GD.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "  r2_scores.append(r2_score(y_test, y_pred_test))\n",
    "  rmse_train_GD.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "  print(\"test RMSE={}\".format(mean_squared_error(y_test, y_pred_test, squared=False)))\n",
    "  print(\"test R2={}\".format(r2_score(y_test, y_pred_test)))\n",
    "  print(\"train RMSE={}\".format(mean_squared_error(y_train, y_pred_train, squared=False)))\n",
    "  print(\"train R2={}\".format(r2_score(y_train, y_pred_train)))\n",
    "\n",
    "best_eta = eta_values[np.argmin(rmse_test_GD)]\n",
    "plt.plot(eta_values,rmse_test_GD,'r--+')\n",
    "plt.title(f\"RMSE VS eta - best eta: {best_eta}\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"eta\")\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve after finding best n value\n",
    "eta = best_eta # learning rate\n",
    "n_iterations = 100\n",
    "m=X_train.shape[0]\n",
    "\n",
    "rmse_train, rmse_test = [], []\n",
    "\n",
    "# coefficients random initialization\n",
    "# Add a column of 1s to X_train to represent the intercept term\n",
    "theta = np.random.randn(X_train.shape[1]+1,1).reshape(1,-1)[0]\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "  # Calculate the predictions\n",
    "  y_predict = X_b_train.dot(theta)\n",
    "  # Calculate the gradients (derivative of MSE)\n",
    "  gradients = 2/m * X_b_train.T.dot(y_predict - y_train)\n",
    "\n",
    "  theta = theta - eta * gradients\n",
    "\n",
    "  y_pred_test = X_b_test.dot(theta)\n",
    "  y_pred_train = X_b_train.dot(theta)\n",
    "\n",
    "  rmse_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "  rmse_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "plt.plot(range(1, 1+n_iterations), rmse_test, label='Test Data RMSE', color='blue')\n",
    "plt.plot(range(1, 1+n_iterations), rmse_train, label='Train Data RMSE', color='red')\n",
    "plt.xlabel('# iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Comparison')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"test RMSE={}\".format(mean_squared_error(y_test, y_pred_test, squared=False)))\n",
    "print(\"test R2={}\".format(r2_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find best n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual implementation\n",
    "eta = 0.15\n",
    "n_epochs = 100\n",
    "m = X_train.shape[0]\n",
    "theta = np.random.randn(X_train.shape[1]+1,1).reshape(1,-1)[0]\n",
    "rmse_test_SGD, rmse_train_SGD = [], []\n",
    "y_pred_test, y_pred_train = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # Generate a random permutation index\n",
    "  permutation = np.random.permutation(m)\n",
    "  # Shuffle consistently the training set and the target (apply the same permutation)\n",
    "  X_b_train_shuffled = X_b_train[permutation]\n",
    "  y_train_shuffled = y_train[permutation]\n",
    "\n",
    "  # Iterate over training set samples\n",
    "  for i in range(m):\n",
    "    xi = X_b_train_shuffled[i]\n",
    "    yi = y_train_shuffled[i]\n",
    "    gradients = 2 * xi.T.dot(xi.dot(theta)-yi)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    " # Calculate predictions at each epoch\n",
    "  y_pred_test = X_b_test.dot(theta)\n",
    "  y_pred_train = X_b_train_shuffled.dot(theta)\n",
    "  # Calculate RMSE at each epoch\n",
    "  rmse_test_SGD.append(mean_squared_error(y_test, y_pred_test, squared = False))\n",
    "  rmse_train_SGD.append(mean_squared_error(y_train_shuffled, y_pred_train, squared = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 1+n_epochs), rmse_test_SGD, label='Test Data RMSE', color='blue')\n",
    "plt.plot(range(1, 1+n_epochs), rmse_train_SGD, label='Train Data RMSE', color='red')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE Comparison')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"test RMSE={}\".format(mean_squared_error(y_test, y_pred_test, squared=False)))\n",
    "print(\"test R2={}\".format(r2_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting polinomial regression \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg =  PolynomialFeatures(degree = 4)\n",
    "X_poli = poly_reg.fit_transform(X)\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(X_poli, y)\n",
    "\n",
    "#visualize linear regression results\n",
    "plt.scatter(X, y, color= 'red')\n",
    "plt.plot(X, lin_reg.predict(X), color = 'blue')\n",
    "plt.title('Reality vs Bluff(linear regression)')\n",
    "plt.xlabel('Position label')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "#visualize polinomial regression results\n",
    "plt.scatter(X, y, color= 'red')\n",
    "plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue')\n",
    "plt.title('Reality vs Bluff(polynomial regression)')\n",
    "plt.xlabel('Position label')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "#Predicting a new result with Linear Regression\n",
    "lin_reg.predict(6.5)\n",
    "\n",
    "#Predicting a new result with Polinomial regression\n",
    "lin_reg_2.predict(poly_reg.fit_transform(6.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas=[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.2]\n",
    "\n",
    "rmse_values=[]\n",
    "\n",
    "for alpha in alphas:\n",
    "  ridge_model = Ridge(alpha=alpha)\n",
    "  ridge_model.fit(X_train_poly, y_train)\n",
    "  y_predict=ridge_model.predict(X_test_poly)\n",
    "  rmse_values.append(np.sqrt(mean_squared_error(y_test, y_predict)))\n",
    "\n",
    "# plt.plot(alphas, rmse_values)\n",
    "# plt.xlabel('alpha')\n",
    "# plt.ylabel(\"RMSE\")\n",
    "\n",
    "# for i, j in zip(alphas, rmse_values):\n",
    "#     print('Alpha = {}, RMSE = {}'.format(i, j))\n",
    "\n",
    "# print(\"Minimum test-RMSE = {}\".format(np.min(rmse_values)))\n",
    "\n",
    "# Plot RMSE values against alpha values\n",
    "plt.plot(alphas, rmse_values, marker='o')\n",
    "plt.xscale('log')  # Use a log scale for better visualization\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Alpha (log scale)')\n",
    "plt.show()\n",
    "\n",
    "# Find the minimum test-RMSE and corresponding alpha\n",
    "min_rmse_index = np.argmin(rmse_values)\n",
    "min_rmse = rmse_values[min_rmse_index]\n",
    "optimal_alpha = alphas[min_rmse_index]\n",
    "\n",
    "print(f'\\nMinimum Test-RMSE: {np.round(min_rmse,4)} at Alpha: {optimal_alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alphas=[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.2]\n",
    "rmse_values=[]\n",
    "\n",
    "for alpha in alphas:\n",
    "  lasso_model = Lasso(alpha=alpha)\n",
    "  lasso_model.fit(X_train, y_train)\n",
    "  y_predict=lasso_model.predict(X_test)\n",
    "  rmse_values.append(np.sqrt(mean_squared_error(y_test, y_predict)))\n",
    "\n",
    "# Plot RMSE values against alpha values\n",
    "plt.plot(alphas, rmse_values, marker='o')\n",
    "plt.xscale('log')  # Use a log scale for better visualization\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs Alpha (log scale)')\n",
    "plt.show()\n",
    "\n",
    "# Find the minimum test-RMSE and corresponding alpha\n",
    "min_rmse_index = np.argmin(rmse_values)\n",
    "min_rmse = rmse_values[min_rmse_index]\n",
    "optimal_alpha = alphas[min_rmse_index]\n",
    "\n",
    "print(f'\\nMinimum Test-RMSE: {np.round(min_rmse,4)} at Alpha: {optimal_alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting decision tree regression model to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X,y)\n",
    "\n",
    "\n",
    "#Predicting a new result with decision tree regression\n",
    "y_pred = regressor.predict(6.5)\n",
    "\n",
    "\n",
    "#visualize polinomial regression results(for high resolution and smoother curves)\n",
    "X_grid = np.arange(min(X), max(X), 0.1)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color= 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Reality vs Bluff(decision tree regression)')\n",
    "plt.xlabel('Position label')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fitting Random forest regression model to the dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "regressor.fit(X,y)\n",
    "\n",
    "\n",
    "#Predicting a new result with Random forest regression regression\n",
    "y_pred = regressor.predict(6.5)\n",
    "\n",
    "\n",
    "#visualize Random forest regression results(for high resolution and smoother curves)\n",
    "X_grid = np.arange(min(X), max(X), 0.1)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(X, y, color= 'red')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "plt.title('Reality vs Bluff(decision tree regression)')\n",
    "plt.xlabel('Position label')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scale features using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Scaled X_train and X_test.\n",
    "    \"\"\"\n",
    "    sc_X = StandardScaler()\n",
    "    X_train_scaled = sc_X.fit_transform(X_train)\n",
    "    X_test_scaled = sc_X.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def fit_logistic_regression(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit Logistic Regression model to the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - y_train (pd.Series): Target variable of the training set.\n",
    "\n",
    "    Returns:\n",
    "    - LogisticRegression: Fitted Logistic Regression model.\n",
    "    \"\"\"\n",
    "    classifier = LogisticRegression(random_state=0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "def predict_and_confusion_matrix(classifier, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Predict test set results and compute the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - classifier (LogisticRegression): Fitted Logistic Regression model.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "    - y_test (pd.Series): Target variable of the test set.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: y_pred (predicted values), cm (confusion matrix).\n",
    "    \"\"\"\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return y_pred, cm\n",
    "\n",
    "def plot_decision_boundary(X_set, y_set, classifier, title):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for the logistic regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_set (pd.DataFrame): Features of the dataset.\n",
    "    - y_set (pd.Series): Target variable of the dataset.\n",
    "    - classifier (LogisticRegression): Fitted Logistic Regression model.\n",
    "    - title (str): Plot title.\n",
    "    \"\"\"\n",
    "    feature_count = X_set.shape[1]\n",
    "    if feature_count == 2:\n",
    "        X1, X2 = np.meshgrid(\n",
    "            np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n",
    "            np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01)\n",
    "        )\n",
    "        plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "                     alpha=0.75, cmap=ListedColormap(('red', 'green')))\n",
    "        plt.xlim(X1.min(), X2.max())\n",
    "        plt.ylim(X2.min(), X1.max())\n",
    "        for i, j in enumerate(np.unique(y_set)):\n",
    "            plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                        c=ListedColormap(('red', 'green'))(i), label=j)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plotting is supported for 2 features only.\")\n",
    "\n",
    "def visualize_results(X_train, y_train, X_test, y_test, classifier):\n",
    "    \"\"\"\n",
    "    Visualize the training and test set results.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - y_train (pd.Series): Target variable of the training set.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "    - y_test (pd.Series): Target variable of the test set.\n",
    "    - classifier (LogisticRegression): Fitted Logistic Regression model.\n",
    "    \"\"\"\n",
    "    # Visualizing the training set results\n",
    "    plot_decision_boundary(X_train, y_train, classifier, title='Logistic Regression (Training Set)')\n",
    "\n",
    "    # Visualizing the test set results\n",
    "    plot_decision_boundary(X_test, y_test, classifier, title='Logistic Regression (Test Set)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scale features using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Scaled X_train and X_test.\n",
    "    \"\"\"\n",
    "    sc_X = StandardScaler()\n",
    "    X_train_scaled = sc_X.fit_transform(X_train)\n",
    "    X_test_scaled = sc_X.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def fit_naive_bayes_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit Naive Bayes Classifier to the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - y_train (pd.Series): Target variable of the training set.\n",
    "\n",
    "    Returns:\n",
    "    - GaussianNB: Fitted Naive Bayes Classifier.\n",
    "    \"\"\"\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "def predict_and_confusion_matrix(classifier, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Predict test set results and compute the confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - classifier (GaussianNB): Fitted Naive Bayes Classifier.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "    - y_test (pd.Series): Target variable of the test set.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: y_pred (predicted values), cm (confusion matrix).\n",
    "    \"\"\"\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return y_pred, cm\n",
    "\n",
    "def plot_decision_boundary(X_set, y_set, classifier, title):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for the Naive Bayes Classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - X_set (pd.DataFrame): Features of the dataset.\n",
    "    - y_set (pd.Series): Target variable of the dataset.\n",
    "    - classifier (GaussianNB): Fitted Naive Bayes Classifier.\n",
    "    - title (str): Plot title.\n",
    "    \"\"\"\n",
    "    feature_count = X_set.shape[1]\n",
    "    if feature_count == 2:\n",
    "        X1, X2 = np.meshgrid(\n",
    "            np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\n",
    "            np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01)\n",
    "        )\n",
    "        plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "                     alpha=0.75, cmap=ListedColormap(('red', 'green')))\n",
    "        plt.xlim(X1.min(), X2.max())\n",
    "        plt.ylim(X2.min(), X1.max())\n",
    "        for i, j in enumerate(np.unique(y_set)):\n",
    "            plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                        c=ListedColormap(('red', 'green'))(i), label=j)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plotting is supported for 2 features only.\")\n",
    "\n",
    "def visualize_results(X_train, y_train, X_test, y_test, classifier):\n",
    "    \"\"\"\n",
    "    Visualize the training and test set results for Naive Bayes Classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Features of the training set.\n",
    "    - y_train (pd.Series): Target variable of the training set.\n",
    "    - X_test (pd.DataFrame): Features of the test set.\n",
    "    - y_test (pd.Series): Target variable of the test set.\n",
    "    - classifier (GaussianNB): Fitted Naive Bayes Classifier.\n",
    "    \"\"\"\n",
    "    # Visualizing the training set results\n",
    "    plot_decision_boundary(X_train, y_train, classifier, title='Naive Bayes (Training Set)')\n",
    "\n",
    "    # Visualizing the test set results\n",
    "    plot_decision_boundary(X_test, y_test, classifier, title='Naive Bayes (Test Set)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting KNN classifier to the training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p=2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#predict test set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#visualizing the training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step=0.01), \n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), \n",
    "             alpha = 0.75, cmap= ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X2.max())\n",
    "plt.ylim(X2.min(), X1.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], \n",
    "                c= ListedColormap(('red', 'green'))(i), label= j)\n",
    "\n",
    "plt.title('K-NN  (training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#visualizing the test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step=0.01), \n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), \n",
    "             alpha = 0.75, cmap= ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X2.max())\n",
    "plt.ylim(X2.min(), X1.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], \n",
    "                c= ListedColormap(('red', 'green'))(i), label= j)\n",
    "\n",
    "plt.title('K-NN (test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "#fitting decision tree classifier to the training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion= 'entropy', \n",
    "                                    random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#predict test set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#visualizing the training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step=0.01), \n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), \n",
    "             alpha = 0.75, cmap= ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X2.max())\n",
    "plt.ylim(X2.min(), X1.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], \n",
    "                c= ListedColormap(('red', 'green'))(i), label= j)\n",
    "\n",
    "plt.title('Decision tree classifier (training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#visualizing the test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step=0.01), \n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), \n",
    "             alpha = 0.75, cmap= ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X2.max())\n",
    "plt.ylim(X2.min(), X1.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], \n",
    "                c= ListedColormap(('red', 'green'))(i), label= j)\n",
    "\n",
    "plt.title('Decision tree classifier (test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the tree\n",
    "# In the terminal enter: pip install pydot2\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(classifier,\n",
    "                     out_file = dot_data,\n",
    "                     feature_names = ['Age', 'Estimated Salary'],\n",
    "                     class_names = ['Yes', 'No'],\n",
    "                     filled = True,\n",
    "                     rounded = True,\n",
    "                     special_characters = True)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fitting random forest classification to the training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, \n",
    "                                    criterion = 'entropy', \n",
    "                                    random_state = 0 )\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#predict test set result\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#visualizing the training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step=0.01), \n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), \n",
    "             alpha = 0.75, cmap= ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X2.max())\n",
    "plt.ylim(X2.min(), X1.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], \n",
    "                c= ListedColormap(('red', 'green'))(i), label= j)\n",
    "\n",
    "plt.title('Random forest classification (training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#visualizing the test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step=0.01), \n",
    "                     np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() + 1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), \n",
    "             alpha = 0.75, cmap= ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X2.max())\n",
    "plt.ylim(X2.min(), X1.max())\n",
    "for i,j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], \n",
    "                c= ListedColormap(('red', 'green'))(i), label= j)\n",
    "\n",
    "plt.title('Random forest classification  (test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
